# summerintership2024_22AIML003

## 14th May 2024
# Summer Internship 2024

## Introduction
This repository documents my experiences, learning, and projects during my summer internship as a Data Analyst, starting from May 14th, 2024, and concluding on July 14th, 2024.

## Internship Overview
My summer internship at Padhya Software Technology began on May 14th, 2024, and will conclude on July 14th, 2024. As a Data Analyst, I am responsible for working with Power BI, Tableau, and Python to analyze and visualize data, providing insights and supporting decision-making processes.

## Responsibilities
As a Data Analyst, my key responsibilities include:

1. **Power BI:**
   - Create and maintain interactive dashboards.
   - Clean and model data for visualization.
   - Design and implement visualizations to convey key insights.

2. **Python:**
   - Perform data analysis and scripting.
   - Automate data extraction, transformation, and loading (ETL) processes.
   - Implement automation scripts to streamline data workflows.
   - Support advanced data analytics and machine learning projects.

3. **General Data Analysis:**
   - Collect, clean, and preprocess data.
   - Analyze datasets to extract meaningful insights.
   - Collaborate with team members to understand data requirements and deliver solutions.
   - Present data findings to stakeholders in a clear and actionable manner.

## Weekly Progress

### Week 1: May 14th - May 17th
- Attended an introductory meeting to understand my role and responsibilities as a Data Analyst. Discussed the overall goals and expectations for the internship.
- Set up my work environment, including installing necessary software such as Power BI, Tableau, and Python.
- Familiarized myself with the company's data infrastructure and reporting tools.
- Started exploring existing dashboards and reports to understand the current state of data visualization in the company.
- Met with team members to discuss ongoing projects and how I can contribute.
- Began initial training sessions on Power BI and Tableau to get up to speed with the tools.

### Week 2: May 20th - May 24th
- This week, I focused on developing a Power BI dashboard to visualize sales data, which was my first major project. I began by familiarizing myself with the Power BI interface, including its various tools and - -features, to create effective and interactive visualizations. My initial task was to connect to the company's sales database, where I performed data extraction, transformation, and loading (ETL) processes to ensure that the data was clean, consistent, and ready for analysis.

- Throughout the week, I explored different visualization options such as bar charts, line graphs, and pie charts to represent sales trends, comparisons, and performance metrics accurately. I learned how to use filters, slicers, and other interactive elements to allow users to drill down into the data for more detailed insights. Additionally, I created calculated columns and measures using DAX (Data Analysis Expressions) to enhance the dashboard's analytical capabilities.

- To ensure that the dashboard met the needs of the end-users, I collaborated closely with team members and stakeholders to understand their specific requirements and the key performance indicators (KPIs) they needed to monitor. This collaboration helped me tailor the dashboard to provide relevant and actionable insights.

- I also spent time learning how to integrate Power BI with other data sources and tools, such as Excel and SQL Server, which broadened my understanding of the data ecosystem within the company. This integration knowledge is crucial for creating comprehensive and connected data solutions.

By the end of the week, I had developed a functional prototype of the sales dashboard. This prototype included various visualizations that effectively communicated sales performance and trends. I presented the prototype to my team for feedback, which included suggestions for improvements and additional features. This iterative process will help refine the dashboard and ensure it meets all user requirements.

![Sales Dashboard](images/sales_dashboard.png)

### Week 3: May 27th - May 31st
- This week, I focused on learning about web scraping and practicing on weather data. Web scraping is the process of extracting data from websites, which can then be used for analysis and reporting. I started by familiarizing myself with the basic concepts of web scraping, including understanding HTML structure, identifying the elements to extract, and using libraries to automate the extraction process.

- I learned to use popular Python libraries such as BeautifulSoup and requests to perform web scraping tasks. BeautifulSoup is used to parse HTML and XML documents, allowing easy navigation and data extraction, while requests is used to send HTTP requests to websites to retrieve their content.

- For my practice project, I chose to scrape weather data from a weather forecasting website. This involved writing Python scripts to send requests to the website, parse the HTML content, and extract relevant information such as temperature, humidity, and weather conditions. I then cleaned and structured the data to make it suitable for analysis.

- Throughout the week, I encountered and resolved several challenges, such as handling dynamic content and dealing with websites that block scraping attempts. I also learned about ethical considerations and legal issues related to web scraping, ensuring that my practices complied with the website's terms of service.

- By the end of the week, I successfully scraped and processed weather data, which I stored in a CSV file for further analysis. This hands-on experience with web scraping not only improved my Python programming skills but also provided valuable insights into data collection from web sources, a crucial skill for data analysts.

### Week 4: June 3rd - June 7th
- This week, I focused on Exploratory Data Analysis (EDA), an essential step in understanding the structure, patterns, and relationships within a dataset. For the first two days, I dedicated my time to learning the fundamentals of EDA. I explored various techniques such as data visualization, summary statistics, and correlation analysis to uncover insights and identify data quality issues. Using Python libraries like pandas, matplotlib, and seaborn, I practiced these techniques on sample datasets to solidify my understanding.

- After grasping the basics of EDA, I began working on my internship project, which involved predicting house prices in Delhi. This project required me to apply my newly acquired EDA skills to analyze the dataset thoroughly. I started by loading the house price data and performing initial data cleaning, such as handling missing values and removing duplicates. I then generated summary statistics and visualizations to understand the distribution of house prices and the relationships between different features, such as location, size, and amenities.

- To gain deeper insights, I conducted correlation analysis to identify which features had the most significant impact on house prices. I created various plots, including scatter plots, box plots, and histograms, to visualize these relationships. These visualizations helped me identify trends and outliers, guiding me in selecting the most relevant features for the prediction model.

- Throughout the week, I documented my findings and prepared the data for the modeling phase, which would follow in the subsequent weeks. This hands-on experience with EDA not only enhanced my analytical skills but also provided a strong foundation for building accurate and reliable predictive models.

### Week 5: June 10th - June 14th
- This week, I delved into the basics of Machine Learning (ML), which is essential for my final internship project on house price prediction. The first few days were dedicated to understanding the fundamental concepts of ML, including supervised and unsupervised learning, training and testing datasets, and model evaluation metrics. I utilized various online resources and tutorials to grasp these concepts and their practical applications.

- Following my introduction to ML, I focused on learning two key algorithms: Random Forest and Linear Regression. Random Forest is an ensemble learning method that operates by constructing multiple decision trees and merging their results to improve accuracy and prevent overfitting. Linear Regression, on the other hand, is a straightforward yet powerful algorithm for predicting a target variable based on one or more predictor variables.

- To solidify my understanding, I practiced implementing these algorithms using Python libraries such as scikit-learn. I started with Linear Regression, where I learned how to fit a model to data, interpret coefficients, and evaluate model performance using metrics like Mean Squared Error (MSE) and R-squared. Next, I moved on to Random Forest, experimenting with hyperparameter tuning and feature importance to enhance model accuracy.

- By the end of the week, I had a solid grasp of both algorithms and was ready to apply them to my internship project. I started training the Delhi home price prediction model using the dataset I had prepared earlier. This involved splitting the data into training and testing sets, fitting the models, and evaluating their performance. This hands-on experience with ML models provided me with practical skills that are crucial for building and refining predictive models.

- This foundational knowledge in ML will be crucial as I continue to work on improving the house price prediction model in the coming weeks.

### Week 6: June 17th - June 21st
- This week, I began exploring new concepts in Machine Learning (ML) that are necessary for my new final internship project: Loan Approval Prediction. This project will span the next three weeks, and my initial focus was on understanding the specific requirements and challenges associated with predicting loan approvals.

- To start, I familiarized myself with the dataset, which includes various features such as applicant income, credit history, loan amount, and other relevant factors. I performed data cleaning and preprocessing tasks to ensure the data was in a suitable format for analysis. This involved handling missing values, encoding categorical variables, and normalizing numerical features.

- Next, I researched different ML algorithms that are commonly used for classification tasks, such as Logistic Regression, Decision Trees, and Support Vector Machines (SVM). I explored their theoretical foundations and practical applications, comparing their strengths and weaknesses in the context of loan approval prediction.

- Throughout the week, I also focused on feature engineering, a crucial step in improving model performance. I generated new features that could potentially enhance the predictive power of the model. For instance, I created derived features such as debt-to-income ratio and loan-to-value ratio, which are important indicators in the loan approval process.

- I also learned about model evaluation metrics specific to classification problems, such as precision, recall, F1-score, and Area Under the ROC Curve (AUC-ROC). Understanding these metrics is vital for assessing the performance of my models and making informed decisions on model selection and tuning.

- By the end of the week, I had a solid understanding of the dataset and the ML techniques required for the Loan Approval Prediction project. This foundational knowledge will guide my work in the upcoming weeks as I build, evaluate, and refine the predictive models.



## Skills Learned
- Proficiency in Power BI for creating and maintaining interactive dashboards.
- Advanced skills in Tableau for data visualization and storytelling.
- Data analysis using Python, including processes and predictive analytics.
- Strong understanding of data cleaning, preparation, and quality assurance.
- Effective communication and presentation skills for sharing insights and findings.

## Conclusion
Overall, this internship has been a valuable learning experience. I gained practical skills in data analysis, visualization, and automation using Power BI, Tableau, and Python. The projects I worked on have provided me with insights into real-world data challenges and solutions. I am grateful for the opportunity to contribute to the team and grow as a Data Analyst.


---

